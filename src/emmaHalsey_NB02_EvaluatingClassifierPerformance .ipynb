{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b509506e-a415-456c-ae32-729f0af13313",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook 2: Evaluating Classifier Performance\n",
    "=============================================\n",
    "\n",
    "## Goals for learning\n",
    "In this assignment, we will:\n",
    "\n",
    "1) Explore methods for calculating classifier performance metrics\n",
    "2) Reflect on the real-world trade-offs that these performance metrics can inform \n",
    "3) Practice working with the array-oriented programming paradigm\n",
    "4) Gain experience working with off-the-shelf machine learning and data analysis libraries\n",
    "\n",
    "## Instructions\n",
    "* Read through the notebook.\n",
    "* Answer any plain text questions (replace cell content, \"YOUR RESPONSE HERE\", with your response).\n",
    "* Insert your code within the code blocks marked with the comments \"# START your code here\" and \"# STOP your code here\".\n",
    "* Do not use loops, iteration, or recursion in any of the code cells.\n",
    "* Do not use any \"Generative AI\" tools or assistants in the creation of your solutions.\n",
    "* Run all cells to make sure your code works and you see reasonable results.\n",
    "\n",
    "## Submission details\n",
    "* Due: Monday 9/15, 11:59 PM\n",
    "* [Submission instructions](https://www.cs.oswego.edu/~agraci2/csc461/submission_instructions.html)\n",
    "\n",
    "## Notebook premise\n",
    "You are a machine learning engineer for a large healthcare company. Your organization needs a model for predicting which patients are likely to have diabetes. Much to your dismay, the financial analysts have decided that it would be much more cost effective to purchase a third-party model than to develop one in-house. You have been tasked with evaluating two different third-party models, one from Company A and one from Company B, and recommending which one your company should purchase based on performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225e8a9-cda0-472b-9e68-4171220b26e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About the dataset\n",
    "The [Pima Indians Diabetes Database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) is a Public Domain data repository provided by the UC Irvine.\n",
    "It contains medical predictor variables that can be used to diagnostically predict whether or not a patient has diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cea5e-5b92-476c-84c1-a89302c52ed5",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> Based on the above description, does this data best support a *binary* or *multiclass* classification task?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "676027c1-8b07-4bd0-87fa-36b72e0eb64c",
   "metadata": {},
   "source": [
    "binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fcbeb-a563-492f-af27-92c1757e2feb",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> Based on the above description, what real-world entity does a row (one item from axis 0) represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83a396e5-441e-49ad-9b4b-4faf4e9598f4",
   "metadata": {},
   "source": [
    "A patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7e827-97e5-49a9-922d-9cc305cb4565",
   "metadata": {},
   "source": [
    "## Loading our data in Python\n",
    "The Python code snippet below shows:\n",
    "\n",
    "1) how to access the database from within a notebook and \n",
    "2) how to read in [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)-formatted data using the [Pandas](https://pypi.org/project/pandas/) library\n",
    "\n",
    "Note: This is the same dataset that we used for Notebook #1.\n",
    "If you need to download the dataset again, please see the \"Dataset: Pima Indians Diabetes Database\" submodule on Brightspace, \n",
    "or download it from [Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3417acb5-8d10-4523-a8d7-e5fb15ea9fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# START your code here\n",
    "DATASET_ROOT_DIR='/home/agraci2/data/' # Please edit with your dataset path\n",
    "# STOP your code here\n",
    "\n",
    "# dataframe = pandas.read_csv(DATASET_ROOT_DIR + 'pima/diabetes.csv')\n",
    "df = pandas.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab0679-012a-4615-b32e-6fd231be403d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use the following cell to further explore the dataset using the [DataFrame](https://pandas.pydata.org/docs/reference/frame.html) datatype's functions, then use the results to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e1e702-1594-4c21-af66-e05a6478e57c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "------------------------------------------------------------\n",
      "Shape:  (768, 9)\n",
      "Size:  6912\n"
     ]
    }
   ],
   "source": [
    "# START your code here\n",
    "pandas.set_option('display.max_columns',None)\n",
    "\n",
    "print(df.head())\n",
    "print('------------------------------------------------------------')\n",
    "print(\"Shape: \",df.shape)\n",
    "print(\"Size: \",df.size)\n",
    "# STOP your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b447d-febc-4ced-91b8-dc41e94debf4",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> How many patients are represented in the database?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebe0af4a-d00b-469d-a3f7-2869eafa75f3",
   "metadata": {},
   "source": [
    "768 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b081e6-8ceb-4eed-8f24-ea32103703ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing our \"Training Set\" and \"Evaluation Set\" from the data\n",
    "\n",
    "Now that we have our data loaded, the next thing we want to do is to partition all of our data into two sets:\n",
    "\n",
    "1) A \"Training Set\" that can be used to train our classifier models.\n",
    "2) An \"Evaluation Set\" (sometimes called a \"Test Set\") that can be used to evaluate our models against \"novel\" data (by which I mean data that the model hasn't seen before).\n",
    "\n",
    "Typically, you want to use the bulk of your data for training in order to get the best results from your model. In this exercise, we will be using an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4f5894-2113-4af8-9080-3bb076b9251d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the training set: 614\n",
      "Number of entries in the evaluation set: 154\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# We want to use an 80/20 split between our training and test sets\n",
    "TRAINING_PERCENTAGE=80\n",
    "\n",
    "# First, we will use the NumPy library to randomly select entries for either the training set or evaluation set\n",
    "np_random = numpy.random.RandomState(seed=12345)\n",
    "rand_unifs = np_random.uniform(0,1,size=df.shape[0]) # A collection of random numbers [0,1) corresponding to each entry in our dataframe\n",
    "division_threshold = numpy.percentile(rand_unifs, TRAINING_PERCENTAGE) # A threshold that the random numbers above can be checked against to see if they fall into the 80% or 20%\n",
    "\n",
    "# The training set will use the first 80% of entries\n",
    "train_indicator = rand_unifs < division_threshold # A collection of True/False indicators corresponding to each entry in our dataframe\n",
    "train_dataframe = df[train_indicator].reset_index(drop=True) # Filter our dataframe based on the training indicators above\n",
    "\n",
    "# The test set will use the remaining 20% of entries\n",
    "eval_indicator = rand_unifs >= division_threshold # A collection of True/False indicators corresponding to each entry in our dataframe (inverse of train_indicator)\n",
    "eval_dataframe = df[eval_indicator].reset_index(drop=True) # Filter our dataframe based on the evaluation indicators above\n",
    "\n",
    "# Show how many entries (rows) are in our training vs evaluation dataframes:\n",
    "print(f'Number of entries in the training set: {len(train_dataframe)}')\n",
    "print(f'Number of entries in the evaluation set: {len(eval_dataframe)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d6efd-1dcc-4afc-a2b3-11c47e56d38c",
   "metadata": {},
   "source": [
    "Use the following cell to further explore the dataset using the [DataFrame](https://pandas.pydata.org/docs/reference/frame.html) datatype's functions, then use the results to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580a1eb4-69a8-4955-b261-ff2b92b02df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "TRAINING SET\n",
      "------------------------------------------------------------\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            1       85             66             29        0  26.6   \n",
      "1            8      183             64              0        0  23.3   \n",
      "2            1       89             66             23       94  28.1   \n",
      "3            0      137             40             35      168  43.1   \n",
      "4            5      116             74              0        0  25.6   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.351   31        0  \n",
      "1                     0.672   32        1  \n",
      "2                     0.167   21        0  \n",
      "3                     2.288   33        1  \n",
      "4                     0.201   30        0  \n",
      "------------------------------------------------------------\n",
      "Shape:  (614, 9)\n",
      "Size:  5526\n",
      "\n",
      "------------------------------------------------------------\n",
      "TEST SET\n",
      "------------------------------------------------------------\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            3       78             50             32       88  31.0   \n",
      "2           10      168             74              0        0  38.0   \n",
      "3            0      118             84             47      230  45.8   \n",
      "4            7      107             74              0        0  29.6   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.248   26        1  \n",
      "2                     0.537   34        1  \n",
      "3                     0.551   31        1  \n",
      "4                     0.254   31        1  \n",
      "------------------------------------------------------------\n",
      "Shape:  (154, 9)\n",
      "Size:  1386\n"
     ]
    }
   ],
   "source": [
    "# START your code here\n",
    "print('\\n------------------------------------------------------------\\nTRAINING SET\\n------------------------------------------------------------')\n",
    "print(train_dataframe.head())\n",
    "print('------------------------------------------------------------')\n",
    "print(\"Shape: \",train_dataframe.shape)\n",
    "print(\"Size: \",train_dataframe.size)\n",
    "print('\\n------------------------------------------------------------\\nTEST SET\\n------------------------------------------------------------')\n",
    "print(eval_dataframe.head())\n",
    "print('------------------------------------------------------------')\n",
    "print(\"Shape: \",eval_dataframe.shape)\n",
    "print(\"Size: \",eval_dataframe.size)\n",
    "# STOP your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76e37f-cec8-431f-945f-d51029f2b925",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Separating features from labels in our training and evaluation sets\n",
    "Remember that in **supervised learning** tasks such as classification, we use **labeled data**. \n",
    "This means that the entries (rows) in our database include **lables** (which can be thought of as a special purpose **feature**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bac3c8-7b26-4fb5-b461-b87909dcd93f",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> Based on what you know from our dataframe object, which columns correspond to our regular input features / predictor variables ($X$)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639cbbac-0eaf-475c-bb34-e54190033d63",
   "metadata": {},
   "source": [
    "Pregnancies, glucose levels, blood pressure, skin thickness, insulin levels, BMI, age, and diabetes pedigree function (DPF, genetic likelihood of developing diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab7f15-f9ea-4448-aecc-c5a62dc5b4e5",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> Based on what you know from our dataframe object, which column corresponds to our output labels ($Y_t$)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13e8180f-4618-4b0b-bcd0-910b97c57072",
   "metadata": {},
   "source": [
    "Outcome (whether or not the patient has diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcffef-6f35-4ac9-8cc9-5c54deb19ec3",
   "metadata": {},
   "source": [
    "##### Next steps:\n",
    "Now that we have our training set and evaluation set partitioned, we will want to separate out the features ($X$) from the labels ($Y_t$) for both sets. The resulting datastructures will be [N-dimensional arrays](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af5b167-1c4b-4651-8c71-2415c0e1702f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (num_rows, num_columns):\n",
      "- Original training data shape: (614, 9)\n",
      "- New training features shape: (614, 8)\n",
      "- New training labels shape: (614,)\n",
      "Evaluation set (num_rows, num_columns):\n",
      "- Original evaluation data shape: (154, 9)\n",
      "- New evaluation features shape: (154, 8)\n",
      "- New evaluation labels shape: (154,)\n"
     ]
    }
   ],
   "source": [
    "# START your code here\n",
    "LABEL_COLUMN='Outcome'\n",
    "# STOP your code here\n",
    "\n",
    "train_features = train_dataframe.loc[:, train_dataframe.columns != LABEL_COLUMN].values # Take all columns except for label\n",
    "train_labels = train_dataframe[LABEL_COLUMN].values # Take only the label column\n",
    "\n",
    "eval_features = eval_dataframe.loc[:, eval_dataframe.columns != LABEL_COLUMN].values # Take all columns except for label\n",
    "eval_labels = eval_dataframe[LABEL_COLUMN].values # Take only the label column\n",
    "\n",
    "print(\"Training set (num_rows, num_columns):\")\n",
    "print(\"- Original training data shape: {}\".format(train_dataframe.shape))\n",
    "print(\"- New training features shape: {}\".format(train_features.shape))\n",
    "print(\"- New training labels shape: {}\".format(train_labels.shape))\n",
    "# Note: the absence of a number indicates an implied \"1\"\n",
    "\n",
    "print(\"Evaluation set (num_rows, num_columns):\")\n",
    "print(\"- Original evaluation data shape: {}\".format(eval_dataframe.shape))\n",
    "print(\"- New evaluation features shape: {}\".format(eval_features.shape))\n",
    "print(\"- New evaluation labels shape: {}\".format(eval_labels.shape)) \n",
    "# Note: the absence of a number indicates an implied \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53080c-e1b8-4f2e-a0bf-f6fdfc8c51b2",
   "metadata": {},
   "source": [
    "## Implementing evaluation functions\n",
    "Now that we have a feel for what our inputs look like and how our data sets are structured, \n",
    "we are ready to implement functions that calculate various **performance metrics**. \n",
    "These metrics will allow us to better understand and compare our classification models.\n",
    "\n",
    "**Please reference the slides posted to Brightspace or your own notes from class to see the equations for each of the performance metrics.**\n",
    "\n",
    "### Implementation instructions\n",
    "* Only place your code between the comments \"# START your code here\" and \"# STOP your code here\".\n",
    "* Do not modify or add code outside of these blocks.\n",
    "* Do not include any libraries other than those provided.\n",
    "* **Do not use loops, iteration, or recursion!**\n",
    "    \n",
    "### Implementation hints\n",
    "* For *TruePositives*, *TrueNegatives*, *FalsePositives*, and *FalseNegatives*, look at the [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#) functions, [NumPy ndarray](https://numpy.org/doc/stable/reference/arrays.ndarray.html#) functions, and [NumPy logical](https://numpy.org/doc/stable/reference/routines.logic.html) functions available to you. Some of the functions I found especially helpful are:\n",
    "    * [numpy.logical_and()](https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html)\n",
    "    * [numpy.logical_or()](https://numpy.org/doc/stable/reference/generated/numpy.logical_or.html)\n",
    "    * [numpy.logical_not()](https://numpy.org/doc/stable/reference/generated/numpy.logical_not.html)\n",
    "    * [numpy.greater()](https://numpy.org/doc/stable/reference/generated/numpy.greater.html)\n",
    "    * [ndarray.sum()](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.sum.html)\n",
    "* Remember that in Python, booleans can be used as integers (False=0, True=1)\n",
    "* For *Accuracy*, *ErrorRate*, *Recall*, *Precision*, and *F1Score*, use the other functions as building blocks.\n",
    "* Feel free to use print statements to explore your intermediate results during development, but please clean them up before submitting.\n",
    "* Farther down in the notebook, I have provided a checkpoint for you to validate your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12693218-4476-4db6-b9f2-d71adf8c9796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Returns an integer (the number of predictions that were true/1 and matched the known label) \n",
    "def TruePositives(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    tp = sum(numpy.logical_and(predictions==1,labels==1))\n",
    "    # STOP your code here\n",
    "    \n",
    "    return tp\n",
    "\n",
    "# Returns an integer (the number of predictions that were true/1 while the known label was false/0) \n",
    "def FalsePositives(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    fp = sum(numpy.logical_and(predictions==1,labels==0))\n",
    "    # STOP your code here\n",
    "    \n",
    "    return fp\n",
    "\n",
    "# Returns an integer (the number of predictions that were false/0 and matched the known label) \n",
    "def TrueNegatives(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    tn = sum(numpy.logical_and(predictions==0,labels==0))\n",
    "    # STOP your code here\n",
    "    \n",
    "    return tn\n",
    "\n",
    "# Returns an integer (the number of predictions that were false/0 while the known label was true/1)\n",
    "def FalseNegatives(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    fn = sum(numpy.logical_and(predictions==0,labels==1))\n",
    "    # STOP your code here\n",
    "    \n",
    "    return fn\n",
    "\n",
    "# Returns a float (the accuracy)\n",
    "def Accuracy(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    tp = TruePositives(predictions, labels)\n",
    "    fp = FalsePositives(predictions, labels)\n",
    "    tn = TrueNegatives(predictions, labels)\n",
    "    fn = FalseNegatives(predictions, labels)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    # STOP your code here\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Returns a float (the error rate)\n",
    "def ErrorRate(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    tp = TruePositives(predictions, labels)\n",
    "    fp = FalsePositives(predictions, labels)\n",
    "    tn = TrueNegatives(predictions, labels)\n",
    "    fn = FalseNegatives(predictions, labels)\n",
    "    er = (fp+fn)/(tp+tn+fp+fn)\n",
    "    # STOP your code here\n",
    "    \n",
    "    return er\n",
    "\n",
    "# Returns a float (the recall/sensitivity)\n",
    "def Recall(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    tp = TruePositives(predictions, labels)\n",
    "    fn = FalseNegatives(predictions, labels)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    # STOP your code here\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "# Returns a float (the Precision)\n",
    "def Precision(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    tp = TruePositives(predictions, labels)\n",
    "    fp = FalsePositives(predictions, labels)\n",
    "    precision = tp/(tp+fp)\n",
    "    # STOP your code here\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def F1Score(predictions, labels):\n",
    "    \n",
    "    # START your code here\n",
    "    sensitivity = Recall(predictions, labels)\n",
    "    precision = Precision(predictions, labels)\n",
    "    f1_score = (2*sensitivity*precision)/(sensitivity+precision)\n",
    "    # STOP your code here\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def PrintPerformanceMetrics(title, predictions, labels):\n",
    "    tp = TruePositives(predictions, labels)\n",
    "    fp = FalsePositives(predictions, labels)\n",
    "    tn = TrueNegatives(predictions, labels)\n",
    "    fn = FalseNegatives(predictions, labels)\n",
    "    print(title)\n",
    "    table = [['', 'Predicted 0', 'Predicted 1', ''], ['Actual 0', f'tn={tn}', f'fp={fp}', f'{tn+fp}'], ['Actual 1', f'fn={fn}', f'tp={tp}', f'{fn+tp}'], ['', f'{tn+fn}', f'{fp+tp}', f'n={tp+fp+tn+fn}']]\n",
    "    # Print the confusion matrix\n",
    "    table = [['', 'Predicted 0', 'Predicted 1', ''], ['Actual 0', f'tn={tn}', f'fp={fp}', f'{tn+fp}'], ['Actual 1', f'fn={fn}', f'tp={tp}', f'{fn+tp}'], ['', f'{tn+fn}', f'{fp+tp}', f'n={tp+fp+tn+fn}']]\n",
    "    print(\"\\n{}\\t\\t|{}\\t|{}\".format(table[0][0], table[0][1], table[0][2]))\n",
    "    print(\"{}\\t|{}\\t\\t|{}\".format(table[1][0], table[1][1], table[1][2]))\n",
    "    print(\"{}\\t|{}\\t\\t|{}\\n\".format(table[2][0], table[2][1], table[2][2]))\n",
    "    # Print computed metrics\n",
    "    print(f'Number of entries in set: {labels.size}')\n",
    "    print(f'Accuracy: {Accuracy(predictions, labels)}')\n",
    "    print(f'Error Rate: {ErrorRate(predictions, labels)}')\n",
    "    print(f'Recall: {Recall(predictions, labels)}')\n",
    "    print(f'Precision: {Precision(predictions, labels)}')\n",
    "    print(f'F1 Score: {F1Score(predictions, labels)}')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258940c-c479-4259-ba4d-396e486cba1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instantiate and train an off-the-shelf classification model (KNN)\n",
    "\n",
    "Company A provides you with their model, which uses a K-Nearest Neighbors (KNN) algorithm.\n",
    "\n",
    "We will explore this algorithm in detail later in the course! For now, we simply want to observe how the model performs, without regard for how it works under the hood.\n",
    "\n",
    "For our evaluation, we will be using an [off-the-shelf implementation of KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) provided by the [scikit-learn](https://scikit-learn.org/stable/index.html) library. Pretend that this is the model implementation provided by Company A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a254bc-c3c7-4d13-bc3f-059376ea4e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 953 ms\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^This will display how long it takes to execute this cell\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "# Grab an off-the-shelf classification model for our first example\n",
    "modelA = sklearn.neighbors.KNeighborsClassifier()\n",
    "\n",
    "# Use our training set (both features and labels) to train the model\n",
    "modelA_trained = modelA.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0705c5c-7741-4873-855b-b39fa08d3b39",
   "metadata": {},
   "source": [
    "## Instantiate and train an off-the-shelf Model (Naive Bayes)\n",
    "\n",
    "Company B provides you with their model, which uses a Naive Bayes algorithm. Again, we only want to observe how the model performs the purposes of this exercise.\n",
    "\n",
    "For our evaluation, we will be using an [off-the-shelf implementation of Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) provided by the [scikit-learn](https://scikit-learn.org/stable/index.html) library. Pretend that this is the implementation provided by Company B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edba6917-5e71-44b5-865e-8d4484607f79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^This will display how long it takes to execute this cell\n",
    "\n",
    "import sklearn.naive_bayes\n",
    "\n",
    "# Grab an off-the-shelf classification model for our second example\n",
    "modelB = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "# Use our training set (both features and labels) to train the model\n",
    "modelB_trained = modelB.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e23204-ba88-44ef-aa3b-9203771599aa",
   "metadata": {},
   "source": [
    "## Evaluating our classifier models\n",
    "Now that our models are trained, we are ready to use them to make predictions.\n",
    "In order to understand how much we can trust the results of these predictions, we will need to understand the performance of our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17411195-8da0-49d5-8c7d-7ef6f003e947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Model A Training Data\n",
      "\n",
      "\t\t|Predicted 0\t|Predicted 1\n",
      "Actual 0\t|tn=362\t\t|fp=43\n",
      "Actual 1\t|fn=75\t\t|tp=134\n",
      "\n",
      "Number of entries in set: 614\n",
      "Accuracy: 0.8078175895765473\n",
      "Error Rate: 0.19218241042345277\n",
      "Recall: 0.6411483253588517\n",
      "Precision: 0.7570621468926554\n",
      "F1 Score: 0.6943005181347152\n",
      "------------------------------------------------------------------------\n",
      "Model A Evaluation Data\n",
      "\n",
      "\t\t|Predicted 0\t|Predicted 1\n",
      "Actual 0\t|tn=81\t\t|fp=14\n",
      "Actual 1\t|fn=32\t\t|tp=27\n",
      "\n",
      "Number of entries in set: 154\n",
      "Accuracy: 0.7012987012987013\n",
      "Error Rate: 0.2987012987012987\n",
      "Recall: 0.4576271186440678\n",
      "Precision: 0.6585365853658537\n",
      "F1 Score: 0.54\n",
      "------------------------------------------------------------------------\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 22.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^This will display how long it takes to execute this cell\n",
    "\n",
    "# Use the first model to make predictions based on the training set\n",
    "modelA_train_pred = modelA_trained.predict(train_features)\n",
    "\n",
    "# Use the first model to make predictions based on the evaluation set\n",
    "modelA_eval_pred = modelA_trained.predict(eval_features)\n",
    "\n",
    "# Next we will use the evaluation metric functions that you implemented earlier and display the results for the first model:\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "PrintPerformanceMetrics('Model A Training Data', modelA_train_pred, train_labels)\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "PrintPerformanceMetrics('Model A Evaluation Data', modelA_eval_pred, eval_labels)\n",
    "print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03bd4e69-0859-443a-9946-e24c7b3d635e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Model B Training Data\n",
      "\n",
      "\t\t|Predicted 0\t|Predicted 1\n",
      "Actual 0\t|tn=346\t\t|fp=59\n",
      "Actual 1\t|fn=84\t\t|tp=125\n",
      "\n",
      "Number of entries in set: 614\n",
      "Accuracy: 0.7671009771986971\n",
      "Error Rate: 0.23289902280130292\n",
      "Recall: 0.5980861244019139\n",
      "Precision: 0.6793478260869565\n",
      "F1 Score: 0.6361323155216285\n",
      "------------------------------------------------------------------------\n",
      "Model B Evaluation Data\n",
      "\n",
      "\t\t|Predicted 0\t|Predicted 1\n",
      "Actual 0\t|tn=80\t\t|fp=15\n",
      "Actual 1\t|fn=23\t\t|tp=36\n",
      "\n",
      "Number of entries in set: 154\n",
      "Accuracy: 0.7532467532467533\n",
      "Error Rate: 0.24675324675324675\n",
      "Recall: 0.6101694915254238\n",
      "Precision: 0.7058823529411765\n",
      "F1 Score: 0.6545454545454547\n",
      "------------------------------------------------------------------------\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 15.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^This will display how long it takes to execute this cell\n",
    "\n",
    "# Use the model to make predictions based on the training set\n",
    "modelB_train_pred = modelB_trained.predict(train_features)\n",
    "\n",
    "# Use the model to make predictions based on the evaluation set\n",
    "modelB_eval_pred = modelB_trained.predict(eval_features)\n",
    "\n",
    "# Next we will use the evaluation metric functions that you implemented earlier and display the results for the second model:\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "PrintPerformanceMetrics('Model B Training Data', modelB_train_pred, train_labels)\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "PrintPerformanceMetrics('Model B Evaluation Data', modelB_eval_pred, eval_labels)\n",
    "print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "145c9ade-97af-44c8-89e1-513ffd308688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Optional Checkpoint\n",
    "\n",
    "# Feel free to use this for validating your results\n",
    "# Note: Because we used the same random seed when shuffling our data, our splits aren't really random.\n",
    "\n",
    "# START your code here\n",
    "enable_checkpoint=True\n",
    "# STOP your code here\n",
    "\n",
    "# If enabled, check just one of our cases (model B on eval data)\n",
    "if enable_checkpoint:\n",
    "    checked_decimals=7\n",
    "    numpy.testing.assert_almost_equal(Accuracy(modelB_eval_pred, eval_labels), 0.7532467532467533, decimal=checked_decimals, verbose=True)\n",
    "    numpy.testing.assert_almost_equal(ErrorRate(modelB_eval_pred, eval_labels), 0.24675324675324672, decimal=checked_decimals, verbose=True)\n",
    "    numpy.testing.assert_almost_equal(Recall(modelB_eval_pred, eval_labels), 0.6101694915254238, decimal=checked_decimals, verbose=True)\n",
    "    numpy.testing.assert_almost_equal(Precision(modelB_eval_pred, eval_labels), 0.7058823529411765, decimal=checked_decimals, verbose=True)\n",
    "    numpy.testing.assert_almost_equal(F1Score(modelB_eval_pred, eval_labels), 0.6545454545454547, decimal=checked_decimals, verbose=True)\n",
    "    print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9eb210-5347-43b0-a2cc-becca2c9d2e1",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> Which model has a greater <u>accuracy</u> when run on **new** data?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88f438db-35be-4101-acb9-fedf13d886c5",
   "metadata": {},
   "source": [
    "The naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e99747-80d5-44fd-8f4a-cf049b2e36f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <b>Question:</b> Assuming your company cares the <u>equally</u> about incorrectly classifying a healthy person as diabetic and incorrectly classifying a diabetic person as healthy, which model would your recommend that your company purchase? Why?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8898dc54-6cd9-437d-bb92-0ff6a9d819d9",
   "metadata": {},
   "source": [
    "The F1 Score, which evaluates both the recall and the precision, indicates that the model our company should purchase is the naive model, since it has a slightly better outcome at 65% vs. kNN's 54%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f345f65-bea1-418f-bdba-c0361270f84e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <b>Question:</b> Based on the above results, which model can more quickly **train** on large datasets?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a1dcb97-f40d-4bec-81f7-343ceeade1c8",
   "metadata": {},
   "source": [
    "The naive bayes model trained much more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ea4f2-718c-4570-8163-8cbc2c358dcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <b>Question:</b> Based on the above results, which model can more quickly make **predictions** at runtime?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebc969fb-5704-4276-b9c3-49305637e99d",
   "metadata": {},
   "source": [
    "The naive bayes, again, predicted more quickly than the kNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b8b39-0a9e-41a6-becf-41bbbe11f825",
   "metadata": {},
   "source": [
    "Before you send out your email to management, making your model recommendation, you decide to double-check your work using **cross-validation**, specifically **K-fold cross validation**, in order to make sure you didn't run into any problems with **overfitting** or **selection bias**.\n",
    "\n",
    "Here are the steps you take:\n",
    "\n",
    "1) Choose an appropriate value of \"K\" to determine the size of your \"folds\".\n",
    "    * If K=1, this is also called \"leave-one-out cross validation\"\n",
    "    * For this task, you decide to define K=8\n",
    "2) Shuffle your entire data set.\n",
    "3) Partition your dataset into N/K partitions:\n",
    "    * Where N is the number of rows in your data.\n",
    "    * Where K is the size of each partition.\n",
    "4) For each partition, re-run your training and evaluation procedure (this is called a \"fold\"):\n",
    "    * Use the current partition as the **evaluation set**.\n",
    "    * Use all other partitions as the **training set**.\n",
    "    * Save off the accuracy for each run.\n",
    "5) Average the results from each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62d1df55-02c6-4658-ab88-9712a9baa9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:8, N:768, P:96\n",
      "Number of partitions: 96\n",
      "Size of each partition: 8\n",
      "Total data size: 768\n",
      "Average accuracy for Model A: 0.71484375\n",
      "Average accuracy for Model B: 0.75390625\n"
     ]
    }
   ],
   "source": [
    "K=8 # Size of each partition\n",
    "N=df.shape[0] # Size of total dataset\n",
    "P=int(N/K) # Number of folds, number of partitions\n",
    "print(\"K:{}, N:{}, P:{}\".format(K,N,P))\n",
    "\n",
    "# Use the NumPy library to \"shuffle\" our data\n",
    "np_random = numpy.random.RandomState(seed=12345)\n",
    "rand_unifs = np_random.uniform(0,1,size=df.shape[0]) # A collection of random numbers [0,1) corresponding to each entry in our dataframe\n",
    "\n",
    "# Create N/K Partitions\n",
    "fold_percentage=100/P\n",
    "partitions=[]\n",
    "for idx in range(0,P):\n",
    "    lower_percentage = idx * fold_percentage\n",
    "    upper_percentage = (idx+1) * fold_percentage\n",
    "    lower_threshold = numpy.percentile(rand_unifs, lower_percentage)\n",
    "    upper_threshold = numpy.percentile(rand_unifs, upper_percentage)\n",
    "    lower_indicator = rand_unifs >= lower_threshold\n",
    "    upper_indicator = rand_unifs <= upper_threshold\n",
    "    part_indicator = numpy.logical_and(lower_indicator, upper_indicator)\n",
    "    part_dataframe = df[part_indicator].reset_index(drop=True)\n",
    "    partitions.append(part_dataframe)\n",
    "    \n",
    "print('Number of partitions: {}'.format(len(partitions)))\n",
    "print('Size of each partition: {}'.format(len(partitions[0])))\n",
    "print('Total data size: {}'.format(len(partitions) * len(partitions[0])))\n",
    "for partition in partitions:\n",
    "    assert len(partition) == len(partitions[0]), len(partition) \n",
    "    \n",
    "# For each partition, re-run training and evaluation\n",
    "AccuraciesModelA=[]\n",
    "AccuraciesModelB=[]\n",
    "for idx in range(0,P):\n",
    "    train_data = pandas.concat([x for i,x in enumerate(partitions) if i!=idx])\n",
    "    eval_data = partitions[idx]\n",
    "    train_features = train_data.loc[:, train_data.columns != LABEL_COLUMN].values\n",
    "    train_labels = train_data[LABEL_COLUMN].values\n",
    "    eval_features = eval_data.loc[:, eval_data.columns != LABEL_COLUMN].values\n",
    "    eval_labels = eval_data[LABEL_COLUMN].values\n",
    "    \n",
    "    modelA_trained = modelA.fit(train_features, train_labels)\n",
    "    modelA_eval_pred = modelA_trained.predict(eval_features)\n",
    "    AccuraciesModelA.append(Accuracy(modelA_eval_pred, eval_labels))\n",
    "    \n",
    "    modelB_trained = modelB.fit(train_features, train_labels)\n",
    "    modelB_eval_pred = modelB_trained.predict(eval_features)\n",
    "    AccuraciesModelB.append(Accuracy(modelB_eval_pred, eval_labels))\n",
    "    \n",
    "# Average results from each fold\n",
    "ModelA_Avg = numpy.mean(AccuraciesModelA)\n",
    "ModelB_Avg = numpy.mean(AccuraciesModelB)\n",
    "print(\"Average accuracy for Model A: {}\".format(ModelA_Avg))\n",
    "print(\"Average accuracy for Model B: {}\".format(ModelB_Avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f7e25-2714-433d-8427-964e3b8fea4a",
   "metadata": {},
   "source": [
    "##### <b>Question:</b> Did the results of the cross-validation change your mind about which model to recommend? Why?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a14c6213-78e7-45e7-a211-8483926fef76",
   "metadata": {},
   "source": [
    "This did not change my mind about the model to recommend: the naive bayes model. It has a superior accuracy AND a superior f1 score, meeting the company's needs better than the kNN model.\n",
    "\n",
    "(Or I would have told them that I could create a model myself and that they don't need contractors to do my job.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17868f9a-d828-4dec-85a9-f3db188444fd",
   "metadata": {},
   "source": [
    "##### Congratulations, you have reached the end of this notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
