{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d272cc1-58f5-4bdd-bfcd-2ade7cd37183",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook 3: Naive Bayes\n",
    "==========================\n",
    "\n",
    "## Goals for learning\n",
    "In this assignment, we will:\n",
    "\n",
    "1) Perform a deep-dive into the Naive Bayes classification algorithm\n",
    "2) Practice working with the array-oriented programming paradigm\n",
    "3) Gain experience working with off-the-shelf machine learning and data analysis libraries\n",
    "\n",
    "## Instructions\n",
    "* Read through the notebook.\n",
    "* Answer any plain text questions (replace cell content, \"YOUR RESPONSE HERE\", with your response).\n",
    "* Insert your code within the code blocks marked with the comments \"# START your code here\" and \"# STOP your code here\".\n",
    "* Do not use loops, iteration, or recursion in any of the code cells.\n",
    "* Do not use any \"Generative AI\" tools or assistants in the creation of your solutions.\n",
    "* Do not import ot use any libraries other than those already imported outside the \"# START your code here\" and \"# STOP your code here\" blocks.\n",
    "* Run all cells to make sure your code works and you see reasonable results.\n",
    "\n",
    "## Submission details\n",
    "* Due: Monday 9/22, 11:59 PM\n",
    "* [Submission instructions](https://www.cs.oswego.edu/~agraci2/csc461/submission_instructions.html)\n",
    "\n",
    "## Notebook premise\n",
    "You have recently been hired as an entry-level ML engineer at a startup. Congratulations!\n",
    "Your startup has been hired by a large healthcare company to implement a diabetes classifier for them.\n",
    "\n",
    "You remember from school that **Naive Bayes** classifiers show competitive performance against other types of classifiers, and are relatively easy to implement. You suggest to your team that you implement a Naive Bayes model, and everyone agrees.\n",
    "\n",
    "Once you get started, you realize that perhaps your professor in college glossed over a few details, and implementing the Naive Bayes won't be quite so simple in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4649a42a-e81e-4c27-8d4a-5b5011c5e891",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive Bayes Implementation\n",
    "\n",
    "### Loading the data set\n",
    "First, we want to load the [Pima Indians Diabetes Database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "Note: This is the same dataset that we used for Notebook #1.\n",
    "If you need to download the dataset again, please see the \"Dataset: Pima Indians Diabetes Database\" submodule on Brightspace, \n",
    "or download it from [Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d81e3e-5b71-4acc-9fa0-6a7a2a2e8d45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import NumPy and Pandas for loading and manipulating data\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "# START your code here\n",
    "# DATASET_ROOT_DIR='/home/agraci2/data/' # Please edit with your dataset path\n",
    "\n",
    "dataframe = pandas.read_csv('diabetes.csv')\n",
    "\n",
    "# STOP your code here\n",
    "\n",
    "# dataframe = pandas.read_csv(DATASET_ROOT_DIR + 'pima/diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5b9d4-e887-41ac-9ce4-86f3b58a7e7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partitioning our data into training and evaluation sets\n",
    "Next, we partition the available data into a training set and an evaluation set, using an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f39ccf-d89d-470f-99fa-7edc704f5495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the training set: 614\n",
      "Number of entries in the evaluation set: 154\n"
     ]
    }
   ],
   "source": [
    "# We want to use an 80/20 split between our training and test sets\n",
    "TRAINING_PERCENTAGE=80\n",
    "\n",
    "# First, we will use the NumPy library to randomly select entries for either the training set or evaluation set\n",
    "np_random = numpy.random.RandomState(seed=12345)\n",
    "rand_unifs = np_random.uniform(0,1,size=dataframe.shape[0]) # A collection of random numbers [0,1) corresponding to each entry in our dataframe\n",
    "division_threshold = numpy.percentile(rand_unifs, TRAINING_PERCENTAGE) # A threshold that the random numbers above can be checked against to see if they fall into the 80% or 20%\n",
    "\n",
    "# The training set will use the first 80% of entries\n",
    "train_indicator = rand_unifs < division_threshold # A collection of True/False indicators corresponding to each entry in our dataframe\n",
    "train_dataframe = dataframe[train_indicator].reset_index(drop=True) # Filter our dataframe based on the training indicators above\n",
    "\n",
    "# The test set will use the remaining 20% of entries\n",
    "eval_indicator = rand_unifs >= division_threshold # A collection of True/False indicators corresponding to each entry in our dataframe (inverse of train_indicator)\n",
    "eval_dataframe = dataframe[eval_indicator].reset_index(drop=True) # Filter our dataframe based on the evaluation indicators above\n",
    "\n",
    "# Show how many entries (rows) are in our training vs evaluation dataframes:\n",
    "print(f'Number of entries in the training set: {len(train_dataframe)}')\n",
    "print(f'Number of entries in the evaluation set: {len(eval_dataframe)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da3693-135b-4181-b099-554dd81e8f02",
   "metadata": {},
   "source": [
    "### Separating features from labels\n",
    "Because our classification model requires **labeled data**, we will separate out the label column ('Outcome') from the non-label features. This transformation will need to be performed on both the training and the evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2503a87-ddf7-4bd6-bb66-21af7a8341c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            3       78             50             32       88  31.0   \n",
       "2           10      168             74              0        0  38.0   \n",
       "3            0      118             84             47      230  45.8   \n",
       "4            7      107             74              0        0  29.6   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.248   26        1  \n",
       "2                     0.537   34        1  \n",
       "3                     0.551   31        1  \n",
       "4                     0.254   31        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4e8c17-7e9d-4a44-9885-87f87ae4d4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# START your code here\n",
    "LABEL_COLUMN = 'Outcome'\n",
    "# STOP your code here\n",
    "\n",
    "train_features = train_dataframe.loc[:, train_dataframe.columns != LABEL_COLUMN].values # Take all columns except for label\n",
    "train_labels = train_dataframe[LABEL_COLUMN].values # Take only the label column\n",
    "\n",
    "eval_features = eval_dataframe.loc[:, eval_dataframe.columns != LABEL_COLUMN].values # Take all columns except for label\n",
    "eval_labels = eval_dataframe[LABEL_COLUMN].values # Take only the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4fb38c-bace-4a7e-8c5b-29f7275f8db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (num_rows, num_columns):\n",
      "- Original training data shape: (614, 9)\n",
      "- New training features shape: (614, 8)\n",
      "- New training labels shape: (614,)\n",
      "Evaluation set (num_rows, num_columns):\n",
      "- Original evaluation data shape: (154, 9)\n",
      "- New evaluation features shape: (154, 8)\n",
      "- New evaluation labels shape: (154,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set (num_rows, num_columns):\")\n",
    "print(\"- Original training data shape: {}\".format(train_dataframe.shape))\n",
    "print(\"- New training features shape: {}\".format(train_features.shape))\n",
    "print(\"- New training labels shape: {}\".format(train_labels.shape))\n",
    "# Note: the absence of a number indicates an implied \"1\"\n",
    "\n",
    "print(\"Evaluation set (num_rows, num_columns):\")\n",
    "print(\"- Original evaluation data shape: {}\".format(eval_dataframe.shape))\n",
    "print(\"- New evaluation features shape: {}\".format(eval_features.shape))\n",
    "print(\"- New evaluation labels shape: {}\".format(eval_labels.shape)) \n",
    "# Note: the absence of a number indicates an implied \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b4b12-7a58-4bfd-89d3-87d63c11faa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Naive Bayes Equation\n",
    "You review your notes from your ML class, and find the simplified equation for determining the predicted labed $y_p$. It returns the label $y$ for which the product of the **conditional probabilities** for each feature, $P(x^i|y)$, and the **prior probability** of the class, $P(y)$, is the highest.\n",
    "\n",
    "$$y_p = \\textsf{argmax}_{y \\in Y}{\\left[\\prod_i^N{P(x^i|y)}\\right]P(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f043b5-0389-4195-b5e3-fa02d79eaad2",
   "metadata": {},
   "source": [
    "### Calculating the prior probability\n",
    "You decide that calculating the **prior probabilities**, $P(y)$, for each class is a good first step.\n",
    "\n",
    "<u>Implementation Hints</u>\n",
    "\n",
    "* For instantiating a NumPy N-Dimensional Array, the [zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) function may be helpful.\n",
    "* I found the NumPy [count_nonzero](https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html) and [logical_not](https://numpy.org/doc/stable/reference/generated/numpy.logical_not.html) functions to be helpful in my implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f68de7-4385-4bc0-833e-246940ab4616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6596091205211726\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# testy testerr\n",
    "\n",
    "num_data_points = train_labels.shape[0]\n",
    "\n",
    "label0 = numpy.count_nonzero(train_labels==0)/num_data_points\n",
    "label1 = numpy.count_nonzero(train_labels)/num_data_points\n",
    "print(label0)\n",
    "prior_probabilities = numpy.array([[label0],[label1]])\n",
    "print(prior_probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01eb34a-fc69-4366-9435-2d44b96f1073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function takes the labels for the training set and determines \n",
    "# the prior probability for each class label\n",
    "def calculate_prior_probabilities(training_labels):\n",
    "    # This is a binary classifier, so there will only be 2 possible label values ('0' and '1')\n",
    "    num_classes = 2\n",
    "    \n",
    "    # There should only be one prior probability per label\n",
    "    prior_per_label = 1\n",
    "    \n",
    "    # There should be one label per data point in our training set\n",
    "    num_data_points = training_labels.shape[0]\n",
    "    \n",
    "    # TODO: Create a 2x1 array, where the first value is the prior probability\n",
    "    # for label '0' and the second value is the prior probability for label '1'\n",
    "    prior_probabilities = None\n",
    "    \n",
    "    # START your code \n",
    "    label0 = numpy.count_nonzero(training_labels==0)/num_data_points\n",
    "    label1 = numpy.count_nonzero(training_labels)/num_data_points\n",
    "    prior_probabilities = numpy.array([[label0],[label1]])\n",
    "    # STOP your code here\n",
    "    \n",
    "    # Use this checkpoint to ensure that your results are in the correct format\n",
    "    assert prior_probabilities.shape == (num_classes, prior_per_label)\n",
    "    \n",
    "    return prior_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c270cb5-e284-4f30-81b3-fd3924eef4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint for prior probability calculations\n",
    "import math\n",
    "priors = calculate_prior_probabilities(train_labels)\n",
    "\n",
    "assert priors.shape == (2, 1)\n",
    "checked_decimals=5\n",
    "numpy.testing.assert_almost_equal(priors[0], 0.65960912, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(priors[1], 0.34039088, decimal=checked_decimals, verbose=True)\n",
    "print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2656767-7b45-4ff1-b2d5-4405b4997d7d",
   "metadata": {},
   "source": [
    "### Calculating conditional probabilities\n",
    "Now that you are on a roll, you decide to tackle the challenge of computing the **conditional probabilities** for each feature, given each class, $P(x^i|y)$.\n",
    "\n",
    "However, once you stop to think about it, the simple **frequency distribution** that your professor used in class (for categorical data types) won't work for your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da7de71f-be32-471f-ae64-e953cf615808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data types in each column\n",
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625e27e-3ad7-4551-95e5-8ffe37983c0f",
   "metadata": {},
   "source": [
    "A quick search from your favorite search engine tells you that using a [**Normal Distribution**](https://en.wikipedia.org/wiki/Normal_distribution), or Gaussian Distribution, is a reasonable intitial choice for calculating distributions with continuous numbers.\n",
    "$$f(x) = {1 \\over \\sigma \\sqrt{2 \\pi}}e^{- {1 \\over 2} {\\left({x-\\mu \\over \\sigma}\\right)^2}}$$\n",
    "\n",
    "Calculating the **conditional probabilities** for each (feature, class) pair with a Gaussian distribution involves calculating the arithmetic **mean** ($\\mu$) and **standard deviation** ($\\sigma$) for each (feature, class) pair. Although the equation looks complicated, you think that you can break down the problem a bit. You decide to start implementing just the mean and standard deviation functions.\n",
    "\n",
    "Equation for Arithmetic Mean:\n",
    "$$\\mu={1 \\over M}\\sum_i^M{x_i}$$\n",
    "\n",
    "Equation for Standard Deviation:\n",
    "$$\\sigma=\\sqrt{{\\sum{(x^i - \\mu)^2}} \\over M}$$\n",
    "\n",
    "<u>Implementation Hints</u>\n",
    "\n",
    "* For instantiating a NumPy N-Dimensional Array, the [zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) function may be helpful.\n",
    "* The NumPy [sum](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.sum.html) function was helpful for my implementation.\n",
    "* Review how arithmetic operators work with NumPy N-Dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8499f28c-8c50-4b93-b7ac-6a9eb73c916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99f5e7f7-fb73-4331-a804-88fc9c751f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_features[:,0],'\\n------------------------------------------------\\n',train_dataframe['Pregnancies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5141d369-e9d8-46a7-96cf-bb880719f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(numpy.mean(train_features[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35872faf-be41-4b6b-9e24-c570bd2b9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.shape(numpy.zeros((8,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03628d31-e7e2-4c01-897b-97334d1daa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 8)\n",
      "(614,)\n"
     ]
    }
   ],
   "source": [
    "# label0 = (train_labels==0)\n",
    "# label1 = (train_labels==1)\n",
    "\n",
    "# ftr0 = train_features[label0]\n",
    "# # print(label0,'\\n',label1)\n",
    "# print(ftr0.shape)\n",
    "# print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7dc96240-c2a5-4f6e-8079-fe1e73fa4864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_means(training_features, training_labels):\n",
    "    # This is a binary classifier, so there will only be 2 possible label values ('0' and '1')\n",
    "    num_classes = 2\n",
    "    \n",
    "    # There should be one known label per data point in our training set\n",
    "    num_data_points = training_labels.shape[0]\n",
    "    assert num_data_points == training_features.shape[0]\n",
    "    \n",
    "    # There should be 8 features for each of our data points\n",
    "    num_features = training_features.shape[1]\n",
    "    assert num_features == 8\n",
    "    \n",
    "    # TODO: Create a 8x2 array, where there is a mean (mu) of the values for each feature\n",
    "    # x_i given each class label y_j\n",
    "    mu = None\n",
    "    \n",
    "    # START your code here\n",
    "    features0 = training_features[training_labels==0]\n",
    "    features1 = training_features[training_labels==1]\n",
    "\n",
    "    mu = numpy.array([features0.mean(axis=0), features1.mean(axis=0)]).T\n",
    "    # STOP your code here\n",
    "    \n",
    "    # Use this checkpoint to ensure that your results are in the correct format\n",
    "    assert mu.shape == (num_features, num_classes)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2b62cbe-ea64-4a67-9bd5-1d5bdd7f429d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint for mean calculations\n",
    "import math\n",
    "mu = calculate_means(train_features, train_labels)\n",
    "assert mu.shape == (8, 2)\n",
    "checked_decimals=5\n",
    "numpy.testing.assert_almost_equal(mu[0][0], 3.48641975, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[0][1], 4.91866029, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[1][0], 109.99753086, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[1][1], 142.30143541, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[2][0], 68.77037037, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[2][1], 70.66028708, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[3][0], 19.51358025, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[3][1], 21.97129187, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[4][0], 66.25679012, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[4][1], 100.55980861, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[5][0], 30.31703704, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[5][1], 35.1492823, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[6][0], 0.42825926, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[6][1], 0.55279904, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[7][0], 31.57283951, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(mu[7][1], 37.39712919, decimal=checked_decimals, verbose=True)\n",
    "print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7618d72-891d-4ce4-9566-4cd8e07a6358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_standard_deviations(training_features, training_labels):\n",
    "    # This is a binary classifier, so there will only be 2 possible label values ('0' and '1')\n",
    "    num_classes = 2\n",
    "    \n",
    "    # There should be one known label per data point in our training set\n",
    "    num_data_points = training_labels.shape[0]\n",
    "    assert num_data_points == training_features.shape[0]\n",
    "    \n",
    "    # There should be 8 features for each of our data points\n",
    "    num_features = training_features.shape[1]\n",
    "    assert num_features == 8\n",
    "    \n",
    "    # TODO: Create a 8x2 array, where there is a mean (mu) of the values for each feature\n",
    "    # x_i given each class label y_j\n",
    "    sigma = None\n",
    "    \n",
    "    # START your code here\n",
    "    features0 = training_features[training_labels==0]\n",
    "    features1 = training_features[training_labels==1]\n",
    "\n",
    "    sigma = numpy.array([features0.std(axis=0), features1.std(axis=0)]).T\n",
    "    # STOP your code here\n",
    "    \n",
    "    # Use this checkpoint to ensure that your results are in the correct format\n",
    "    assert sigma.shape == (num_features, num_classes)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7405d44d-717c-41ad-bcb9-89223d586fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint for standard deviation calculations\n",
    "sigma = calculate_standard_deviations(train_features, train_labels)\n",
    "assert sigma.shape == (8, 2)\n",
    "checked_decimals=5\n",
    "numpy.testing.assert_almost_equal(sigma[0][0], 3.1155426, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[0][1], 3.75417931, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[1][0], 25.96811899, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[1][1], 32.50910874, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[2][0], 18.07540068, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[2][1], 21.69568568, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[3][0], 15.02320635, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[3][1], 17.21685884, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[4][0], 95.63339586, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[4][1], 139.24364214, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[5][0], 7.50030986, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[5][1], 6.6625219, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[6][0], 0.29438217, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[6][1], 0.37201494, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[7][0], 11.67577435, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(sigma[7][1], 11.01543899, decimal=checked_decimals, verbose=True)\n",
    "print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b6831-1a0d-41ec-9727-24104ee986ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that you have the building blocks ($\\mu$ and $\\sigma$) for calculating the Gaussian distributions, you decided to plug it into the distribution equation and try it out on the evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce979bc9-23fd-46b5-baaa-3727d4388e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This calculates the gaussian distribution for a given class\n",
    "def calculate_gaussian_dist_y(X, mu, sigma, y):\n",
    "    mu_y = mu[:,y]\n",
    "    sigma_y = sigma[:,y]\n",
    "    return 1.0 / (numpy.sqrt(2.0 * numpy.pi) * sigma_y) * numpy.exp(-numpy.power((X - mu_y) / sigma_y, 2.0) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff8609c9-f9fb-449f-a41f-3051029b7b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for each feature x_i: [0.09247705 0.00526529 0.0217215  0.01561001 0.00328148 0.0483312\n",
      " 1.0790168  0.00983435]\n",
      "Product of probabilities for all features in X: 2.7785647377592095e-13\n"
     ]
    }
   ],
   "source": [
    "gaussian_dist = calculate_gaussian_dist_y(eval_features, mu, sigma, 0)\n",
    "\n",
    "# Display probabilities for the first evaluation data sample\n",
    "print(\"Probabilities for each feature x_i: {}\".format(gaussian_dist[0]))\n",
    "\n",
    "# Display the product of probabilities for the first evaluation data sample\n",
    "print(\"Product of probabilities for all features in X: {}\".format(numpy.prod(gaussian_dist[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fbf32-c0ae-4b8b-9f76-369ec62894bc",
   "metadata": {},
   "source": [
    "### The log trick\n",
    "You notice something interesting: some of the probability values you are getting are very small (close to zero). You realize that if you multiply many of these very small numbers together as indicated by your equation, they might not be representable as floating point numbers, and might be represented as zero!\n",
    "\n",
    "$$y_p = \\textsf{argmax}_{y \\in Y}{\\left[\\prod_i^N{P(x^i|y)}\\right]P(y)}$$\n",
    "\n",
    "If that happens, your calculations for each class will be zero, and you will have no way to compare the relative probability of an item belonging to one class or another.\n",
    "\n",
    "What can you do about this?\n",
    "\n",
    "Since you are a bit stuck at this point, you reach out to your new work friend and describe the problem. She lends you one of her favorite ML books, and hopes that there might be something in there that can help:\n",
    "> [Forsyth, David. (2019). Applied Machine Learning. 10.1007/978-3-030-18114-7.](https://link.springer.com/book/10.1007/978-3-030-18114-7)\n",
    "\n",
    "In this book, the author suggests an approach to dealing with the problem of small numbers.\n",
    "By leveraging the [monotonic](https://en.wikipedia.org/wiki/Monotonic_function) property of [logarithms](https://en.wikipedia.org/wiki/Logarithm) (namely that when $a > b$ then $log(a) > log(b)$ also), you can do your calculations with much larger numbers, while ensuring that you are finding the class with the greatest probability.\n",
    "\n",
    "$$\\textsf{argmax}_{y \\in Y}{\\left[\\prod_i^N{P(x^i|y)}\\right]P(y)} = \\textsf{argmax}_{y \\in Y}{log \\left(\\left[\\prod_i^N{P(x^i|y)}\\right]P(y)]\\right)}$$\n",
    "\n",
    "Combining this property with the fact that the log of a product is the sum of the logs (i.e., $log_a(xy) = log_a(x) + log_a(y)$), leads to an alternative representation of our Naive Bayes equation that adds logarithms of probabilities, rather than multiplying the probabilities:\n",
    "\n",
    "$$y_p = \\textsf{argmax}_{y \\in Y}{\\left[\\sum_i^N{log(P(x^i|y))}\\right]+log(P(y))}$$\n",
    "\n",
    "You decide that this approach looks promising, and make a mental note to thank your new work friend later.\n",
    "\n",
    "<u>Implementation Hint</u>\n",
    "\n",
    "* The [NumPy \"log\" function](https://numpy.org/doc/stable/reference/generated/numpy.log.html) can be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f0eeecf9-2ad4-4b84-ba8e-1e4a613e886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features0 = train_labels==0\n",
    "# features1 = train_labels==1\n",
    "\n",
    "# prior_probabilities = calculate_prior_probabilities(train_labels)\n",
    "# log_prior_probabilities = numpy.array(numpy.log(prior_probabilities))\n",
    "\n",
    "# print(log_prior_probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28ac606a-1105-40fb-adb1-db4b202f5b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the log of the prior probabilities for each class\n",
    "def calculate_log_prior_probabilities(training_labels):\n",
    "    # You can use the function you already wrote to get the prior probabilities\n",
    "    prior_probabilities = calculate_prior_probabilities(training_labels)\n",
    "    \n",
    "    # TODO: Create a 2x1 array, where the first value is the log of the prior probability\n",
    "    # for label '0' and the second value is the log of the prior probability for label '1'\n",
    "    log_prior_probabilities = None\n",
    "    \n",
    "    # START your code here\n",
    "    log_prior_probabilities = numpy.array(numpy.log(prior_probabilities))\n",
    "    # STOP your code here\n",
    "    \n",
    "    # Use this checkpoint to ensure that your results are in the correct format\n",
    "    assert log_prior_probabilities.shape == (2,1)\n",
    "    \n",
    "    return log_prior_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19495fb5-0a27-48d2-8660-c6126d0515e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint for log prior probability calculations\n",
    "import math\n",
    "log_priors = calculate_log_prior_probabilities(train_labels)\n",
    "assert log_priors.shape == (2, 1)\n",
    "checked_decimals=5\n",
    "numpy.testing.assert_almost_equal(log_priors[0], -0.41610786, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(log_priors[1], -1.07766068, decimal=checked_decimals, verbose=True)\n",
    "print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370fddd-d33d-4e5d-aeb2-cccc4e7608e5",
   "metadata": {},
   "source": [
    "You decide to go return your friend's book, and tell her the good news about logarithms.\n",
    "When she hears the approach you are taking, it reminds her that there may be some existing functions you can repuropse from an old project!\n",
    "\n",
    "These functions calculate the log of the gaussian distribution when given:\n",
    "\n",
    "* A feature vector ($X$)\n",
    "* The means for each feature ($\\mu$), given a class $y$\n",
    "* The standard deviations for each feature ($\\sigma$), given a class $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9889950c-8a60-4333-883d-7fcc6211b404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This calculates the log gaussian distribution for a given class\n",
    "def calculate_log_gaussian_dist_y(X, mu, sigma, y):\n",
    "    mu_y = mu[:,y]\n",
    "    sigma_y = sigma[:,y]\n",
    "    first_half = numpy.log(1 / (sigma_y * math.sqrt(2 * math.pi)))\n",
    "    second_half = 0.5 * pow(((X - mu_y) / sigma_y), 2)\n",
    "    return first_half - second_half\n",
    "\n",
    "# This calculates the log gaussian distribution for every class\n",
    "def calculate_log_gaussian_dist(X, mu, sigma):\n",
    "    M = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    log_p_x_y_arr = numpy.zeros((M,N,2))\n",
    "    log_p_x_y_arr[:,:,0] = calculate_log_gaussian_dist_y(X, mu, sigma, 0)\n",
    "    log_p_x_y_arr[:,:,1] = calculate_log_gaussian_dist_y(X, mu, sigma, 1)\n",
    "    return log_p_x_y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ed0c8e1b-2066-4ae3-b67a-de7e66a93820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for each feature x_i: [-2.38079475 -5.2466187  -3.82945292 -4.15984316 -5.71946084 -3.02967803\n",
      "  0.07605026 -4.62187357]\n",
      "Sum of probabilities for all features in X: -28.91167169592075\n"
     ]
    }
   ],
   "source": [
    "log_gaussian_dist = calculate_log_gaussian_dist_y(eval_features, mu, sigma, 0)\n",
    "\n",
    "# Display probabilities for the first evaluation data sample\n",
    "print(\"Probabilities for each feature x_i: {}\".format(log_gaussian_dist[0]))\n",
    "\n",
    "# Display the product of probabilities for the first evaluation data sample\n",
    "print(\"Sum of probabilities for all features in X: {}\".format(numpy.sum(log_gaussian_dist[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3b713-c87b-4274-832f-66b06408d28f",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a5d75-c6b0-4d27-8c92-6f096039193f",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now that you have everything you need to implement the new log-based rule for Naive Bayes, all that is left is to put the pieces together.\n",
    "\n",
    "Remember that this is the equation we are trying to solve:\n",
    "$$y_p = \\textsf{argmax}_{y \\in Y}{\\left[\\sum_i^N{log(P(x^i|y))}\\right]+log(P(y))}$$\n",
    "\n",
    "The function below, \"calculate_log_p_x_y\" should calculate the following for each of our class labels $y$:\n",
    "\n",
    "$${\\left[\\sum_i^N{log(P(x^i|y))}\\right]+log(P(y))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "64f4fb4e-327e-4854-a0ca-1b1a92700f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_log_p_x_y(X, mu, sigma, log_prior):\n",
    "    # This is a binary classifier, so there will only be 2 possible label values ('0' and '1')\n",
    "    num_classes = 2\n",
    "    \n",
    "    # There should be one known label per data point in our training set\n",
    "    num_data_points = X.shape[0]\n",
    "    \n",
    "    # There should be 8 features for each of our data points\n",
    "    num_features = X.shape[1]\n",
    "    assert num_features == 8\n",
    "    \n",
    "    # Use the functions that your work friend gave you to get log(P(x | y)) for x in X\n",
    "    # Note: I am naming this variable \"log_p_x_y_arr\" because it has entries for each of \n",
    "    # the 8 features separated out (features are an array). It is your job to add them up!\n",
    "    log_p_x_y_arr = calculate_log_gaussian_dist(X, mu, sigma)\n",
    "    assert log_p_x_y_arr.shape == (num_data_points, num_features, num_classes)\n",
    "    \n",
    "    # TODO: Create an Mx2 array, where for each data item, you calculate\n",
    "    # the sum of the logs of the conditional probabilities for each feature \n",
    "    # plus the prior probability for the class:\n",
    "    # log(P(x0 | y)) + log(P(x1 | y)) + ... + log(P(xN | y)) + P(y)\n",
    "    log_p_x_y = None\n",
    "    \n",
    "    # START your code here\n",
    "    log_p_x_y = log_p_x_y_arr.sum(axis=1) + log_prior.flatten()\n",
    "    # STOP your code here\n",
    "    \n",
    "    assert log_p_x_y.shape == (num_data_points,num_classes)\n",
    "    return log_p_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1f2e0bbd-566a-4990-879d-f1bdfa60ff2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint for log_p_x_y calculations\n",
    "mu = calculate_means(train_features, train_labels)\n",
    "sigma = calculate_standard_deviations(train_features, train_labels)\n",
    "log_prior = calculate_log_prior_probabilities(train_labels)\n",
    "log_p_x_y = calculate_log_p_x_y(train_features, mu, sigma, log_prior)\n",
    "\n",
    "assert log_p_x_y.shape == (614, 2)\n",
    "checked_decimals=5\n",
    "numpy.testing.assert_almost_equal(log_p_x_y[0][0], -26.96647828, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(log_p_x_y[0][1], -31.00418408, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(log_p_x_y[613][0], -26.98605248, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(log_p_x_y[613][1], -30.80571318, decimal=checked_decimals, verbose=True)\n",
    "print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe0afd-daa9-445c-9c1b-a2fd2cdaae07",
   "metadata": {},
   "source": [
    "Finally, you decide to wrap you classification model up as a class so that it is easier for your team and your customers to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "63baaa59-a768-4581-b462-2b13abc6039a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, training_features, training_labels):\n",
    "        \n",
    "        # The following 3 lines constitute the training step\n",
    "        # We are creating our model from the cached probability values\n",
    "        self.log_priors = calculate_log_prior_probabilities(training_labels)\n",
    "        self.mu = calculate_means(training_features, training_labels)\n",
    "        self.sigma = calculate_standard_deviations(training_features, training_labels)\n",
    "        \n",
    "    def predict(self, features):\n",
    "        \n",
    "        # Use the cached probability values along with the observed values of \n",
    "        # the input feature vectors to make predictions\n",
    "        log_p_x_y = calculate_log_p_x_y(features, self.mu, self.sigma, self.log_priors)\n",
    "        return log_p_x_y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed871c62-f291-4174-bd69-8cf6f2c0198a",
   "metadata": {},
   "source": [
    "Now it is time to try your classifier out!\n",
    "Below, we use your implementation to make predictions on the training and the evaluation data sets, and we calculate accuracy for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "05d9c129-ee00-4e14-b9e3-3048c1d81f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes_classifier = NaiveBayesClassifier(train_features, train_labels)\n",
    "train_prediction = diabetes_classifier.predict(train_features)\n",
    "eval_prediction = diabetes_classifier.predict(eval_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9dede834-0b44-4781-8c6d-359ebe5832ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7671009771986971, Evaluation accuracy: 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = (train_prediction==train_labels).mean()\n",
    "eval_accuracy = (eval_prediction==eval_labels).mean()\n",
    "print(\"Training accuracy: {}, Evaluation accuracy: {}\".format(train_accuracy, eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ef092a15-2441-475f-b434-160f529b2c56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed checkpoint!\n"
     ]
    }
   ],
   "source": [
    "checked_decimals=7\n",
    "numpy.testing.assert_almost_equal(train_accuracy, 0.7671009771986971, decimal=checked_decimals, verbose=True)\n",
    "numpy.testing.assert_almost_equal(eval_accuracy, 0.7532467532467533, decimal=checked_decimals, verbose=True)\n",
    "print(\"Passed checkpoint!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05505e77-a69d-4ee9-9b96-b059b2a59f05",
   "metadata": {},
   "source": [
    "Congratulations, you have reached the end of this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcadf14-6173-4d47-b3b7-ea44125722d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
