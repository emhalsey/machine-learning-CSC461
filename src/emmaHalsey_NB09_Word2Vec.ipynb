{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1600351e-2b0f-4db1-8d04-59217598f4b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook 9: Word2Vec\n",
    "=======================================\n",
    "\n",
    "## Goals for learning\n",
    "In this assignment, we will:\n",
    "1) Practice with the **[PyTorch](https://pytorch.org/)** library for working with neural networks.\n",
    "2) Perform a deep-dive into the Word2Vec algorithm (CBOW variant).\n",
    "3) Examine some applications of word embeddings.\n",
    "\n",
    "## Instructions\n",
    "* Read through the notebook.\n",
    "* Answer any plain text questions (replace cell content, \"YOUR RESPONSE HERE\", with your response).\n",
    "* Insert your code within the code blocks marked with the comments \"# START your code here\" and \"# STOP your code here\".\n",
    "* Do not use any \"Generative AI\" tools or assistants in the creation of your solutions.\n",
    "* Do not import or use any libraries other than those already imported outside the \"# START your code here\" and \"# STOP your code here\" blocks.\n",
    "* Run all cells to make sure your code works and you see reasonable results.\n",
    "    * All code cells should have output indicating the results of the last run when the notebook is submitted.\n",
    "    * If there are errors, or if a code cell does not have output as submittted, points will be deducted.\n",
    "\n",
    "## Submission details\n",
    "* Due: Monday 11/17, 11:59 PM\n",
    "* [Submission instructions](https://www.cs.oswego.edu/~agraci2/csc461/submission_instructions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedff76c-54fd-4f00-bd45-e7ac572d0f40",
   "metadata": {},
   "source": [
    "## Python library dependencies\n",
    "* [Pytorch](https://pytorch.org/) - \"PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63ce476-3197-48e9-a30c-4c4891ab36a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26a953e5870>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90ea86-3e9a-465c-9b52-e2adc14a81f1",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The algorithm we are investigating today can take a long time to train, if given a large corpus to train from.\n",
    "\n",
    "For this notebook, we will only be looking at a very small excerpt of text.\n",
    "\n",
    "This excerpt was taken from [\"The Bird Book\" by Chester A. Reed (2009) via Project Gutenberg](https://www.gutenberg.org/cache/epub/30000/pg30000.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b916f58c-bef7-4175-9269-a7867d427281",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = \"\"\"Unlike the Grebes, Loons do not build in colonies, generally not more\n",
    "than one, or at the most two pairs nesting on the same lake or pond;\n",
    "neither do they seek the marshy sloughs in which Grebes dwell,\n",
    "preferring the more open, clear bodies of water. The common Loon may be\n",
    "known in summer by the entirely black head and neck with the complete\n",
    "ribbon of black and white stripes encircling the lower neck and the\n",
    "narrower one which crosses the throat. The back is spotted with white.\n",
    "In some sections Loons build no nest, simply scooping a hollow out in\n",
    "the sand, while in other places they construct quite a large nest of\n",
    "sticks, moss and grasses. It is usually placed but a few feet from the\n",
    "waters edge, so that at the least suspicion the bird can slide off its\n",
    "eggs into the water, where it can cope with any enemy. The nests are\n",
    "nearly always concealed under the overhanging bushes that line the\n",
    "shore; the one shown in the full page illustration, however, was located\n",
    "upon the top of an old muskrat house. The two eggs which they lay are a\n",
    "very dark greenish brown in color, with black spots.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5090f-d117-4f87-94e6-0ecdfebfbee1",
   "metadata": {},
   "source": [
    "## Cleaning our data\n",
    "Before we learn from our data (corpus), we first want to perform a small amount of cleaning.\n",
    "\n",
    "In the following \"clean_corpus\" function, perform the following transformations:\n",
    "1) Convert all text to lower case.\n",
    "2) Remove the following special symbols: \".\", \"--\", \",\", \";\", \":\", \"_\", \"?\", \"!\"\n",
    "3) Remove the word \"the\" (note: this is a very common [stop word](https://en.wikipedia.org/wiki/Stop_word) to remove in natural language processing (NLP) tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb6f492-1d65-4abe-b4c5-cb5b58a9b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37d2510-7c86-4217-b2d7-799c48c5ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlike grebes loons do not build in colonies generally not more\n",
      "than one or at most two pairs nesting on same lake or pond\n",
      "neither do they seek marshy sloughs in which grebes dwell\n",
      "preferring more open clear bodies of water common loon may be\n",
      "known in summer by entirely black head and neck with complete\n",
      "ribbon of black and white stripes encircling lower neck and the\n",
      "narrower one which crosses throat back is spotted with white\n",
      "in some sections loons build no nest simply scooping a hollow out in\n",
      "sand while in other places they construct quite a large nest of\n",
      "sticks moss and grasses it is usually placed but a few feet from the\n",
      "waters edge so that at least suspicion bird can slide off its\n",
      "eggs into water where it can cope with any enemy nests are\n",
      "nearly always concealed under overhanging bushes that line the\n",
      "shore one shown in full page illustration however was located\n",
      "upon top of an old muskrat house two eggs which they lay are a\n",
      "very dark greenish brown in color with black spots\n"
     ]
    }
   ],
   "source": [
    "def clean_corpus(corpus):\n",
    "    # START your code here\n",
    "    \n",
    "    # 1\n",
    "    corpus = corpus.lower()\n",
    "    # print(corpus,'\\n')\n",
    "    \n",
    "    # 2 - https://docs.python.org/3/library/re.html#\n",
    "    corpus = re.sub(r'[^\\w\\s]', '', corpus)\n",
    "    # print(corpus,'\\n')\n",
    "\n",
    "    # 3\n",
    "    corpus = re.sub(r'the ', '', corpus)\n",
    "    # print(corpus)\n",
    "    # STOP your code here\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = clean_corpus(raw_corpus)\n",
    "print(corpus)\n",
    "\n",
    "# Simple checkpoint\n",
    "# assert len(corpus) == 995, \"Unexpected corpus length: {}\".format(len(corpus)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad64065-964f-4d34-afb9-a3c114133700",
   "metadata": {},
   "source": [
    "## Building our vocabulary\n",
    "The next thing we need to do is to build our vocabulary by **tokenizing** our cleaned corpus.\n",
    "This means we break down our text into individual \"tokens\", which are usually words or phrases.\n",
    "\n",
    "For this notebook, we are going to keep things simple and tokenize by splitting on  white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67207cc4-ddfe-44b3-a744-4e7fb1104dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = None\n",
    "\n",
    "'''\n",
    "TODO: Split the cleaned corpus into tokens, using white space as a delimiter.\n",
    "'''\n",
    "# START your code here\n",
    "tokenized = corpus.split()\n",
    "# print(tokenized)\n",
    "# STOP your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6902eb1e-4b85-482e-806e-aaeece6481c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "vocabulary = None\n",
    "\n",
    "'''\n",
    "TODO: Create a vocabulary by finding the set of all unique tokens.\n",
    "'''\n",
    "# START your code here\n",
    "# https://www.geeksforgeeks.org/python/python-get-unique-values-list/\n",
    "vocabulary = list(dict.fromkeys(tokenized))\n",
    "# STOP your code here\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "assert vocabulary_size == 134, \"Unexpected vocabulary size: {}\".format(vocabulary_size)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ddbda-7c6c-4d7a-ab71-72f382f8449f",
   "metadata": {},
   "source": [
    "## Encoding our text input\n",
    "In order to train on our text data, we need a way to represent our text numerically.\n",
    "In our lecture, we examined one-hot encoding for text data, but because we are working with such a small sample of data, and want to keep things simple, we will be encoding out text by simply mapping each word to a unique ID number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74e9532-5b20-4578-8af3-b85483156c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "'''\n",
    "TODO: Create a map from each word in the vocabulary to a unique ID number, and another map from ID to word.\n",
    "'''\n",
    "# START your code here\n",
    "ids = range(vocabulary_size)\n",
    "\n",
    "word_to_id = dict(zip(vocabulary, ids))\n",
    "id_to_word = dict(zip(ids, vocabulary))\n",
    "# STOP your code here\n",
    "\n",
    "assert isinstance(word_to_id, dict)\n",
    "assert len(word_to_id) == vocabulary_size\n",
    "assert isinstance(id_to_word, dict)\n",
    "assert len(id_to_word) == vocabulary_size\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be084b-d4f8-4115-a0a4-e3a16614b282",
   "metadata": {},
   "source": [
    "## Establishing our window of context words\n",
    "When running Word2Vec, one of the parameters provided to you is the window size. \n",
    "This determines how much context to consider when processing each target word.\n",
    "\n",
    "For example, a window size of 5 would look at 2 words context words before the target word, the target word, and 2 context words after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9857efd6-0f41-4aed-bf6e-4027387ef23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Emma test\n",
    "# counter = 0\n",
    "# WINDOW_SIZE = 5\n",
    "# batches = []\n",
    "# idx_first_full_window = int((WINDOW_SIZE - 1) / 2)\n",
    "# print(\"idx_first_full_window: \", idx_first_full_window,'\\n')\n",
    "# idx_last_full_window = int(len(tokenized) - (WINDOW_SIZE - 1) / 2)\n",
    "# print(\"idx_last_full_window: \", idx_last_full_window,'\\n')\n",
    "# for target_idx in range(idx_first_full_window, idx_last_full_window):\n",
    "#     print('---------------------------------------------------------------------------\\n',counter,'\\n','---------------------------------------------------------------------------\\n')\n",
    "#     target = tokenized[target_idx]\n",
    "#     print(\"target: \", target, '\\n')\n",
    "#     context = [tokenized[target_idx-1], tokenized[target_idx+1]]\n",
    "#     print(\"context: \", context,'\\n')\n",
    "#     batches.append((context, target))\n",
    "#     print(\"batches: \", batches, '\\n')\n",
    "#     counter += 1\n",
    "\n",
    "# https://pythonguides.com/pytorch-dataloader/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fce1296-24e3-4cb7-9857-1cd2af74093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example batch: (['build', 'in', 'generally', 'not'], 'colonies')\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "\n",
    "def get_context_words(tokenized, target_id, WINDOW_SIZE):\n",
    "    context = None\n",
    "    \n",
    "    '''\n",
    "    TODO: Find the context words that come before and after the target word in the tokenized text.\n",
    "    '''\n",
    "    # START your code here\n",
    "    half_window = (WINDOW_SIZE - 1) // 2\n",
    "    context = tokenized[target_id - half_window : target_id] + \\\n",
    "              tokenized[target_id + 1 : target_id + half_window + 1]\n",
    "    # STOP your code here\n",
    "    \n",
    "    assert len(context) == WINDOW_SIZE - 1\n",
    "    return context\n",
    "\n",
    "def batch_data(tokenized, WINDOW_SIZE):\n",
    "    batches = []\n",
    "    idx_first_full_window = int((WINDOW_SIZE - 1) / 2)\n",
    "    idx_last_full_window = int(len(tokenized) - (WINDOW_SIZE - 1) / 2)\n",
    "    for target_idx in range(idx_first_full_window, idx_last_full_window):\n",
    "        target = tokenized[target_idx]\n",
    "        context = get_context_words(tokenized, target_idx, WINDOW_SIZE)\n",
    "        batches.append((context, target))\n",
    "    return batches\n",
    "\n",
    "input_batches = batch_data(tokenized, WINDOW_SIZE)\n",
    "print(\"Example batch: {}\".format(input_batches[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14622470-de50-4f38-8f27-b61908759464",
   "metadata": {},
   "source": [
    "Now that we have our input batches, we will need a utility function to facilitate converting our vector of context words into a vector of numeric IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8029028b-40c1-4d62-83c2-78aafa39b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['build', 'in', 'generally', 'not']\n",
      "tensor([5, 6, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "def context_words_to_ids(context, word_to_id):\n",
    "    context_ids = None\n",
    "\n",
    "    '''\n",
    "    TODO: Convert context words to their IDs\n",
    "    '''\n",
    "    # START your code here\n",
    "    context_ids = [word_to_id[word] for word in context] # mapping\n",
    "    context_ids = torch.tensor(context_ids, dtype=torch.long)\n",
    "    # STOP your code here\n",
    "    \n",
    "    return context_ids\n",
    "\n",
    "example_batch_context_words = input_batches[5][0]\n",
    "example_batch_context_ids = context_words_to_ids(example_batch_context_words, word_to_id)\n",
    "\n",
    "# Simple checkpoint\n",
    "assert isinstance(example_batch_context_ids, torch.Tensor)\n",
    "assert len(example_batch_context_ids) == len(example_batch_context_words)\n",
    "\n",
    "print(example_batch_context_words)\n",
    "print(example_batch_context_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d4319-7528-4082-90e8-8622aabeb5db",
   "metadata": {},
   "source": [
    "## Creating our CBOW model (Neural Network)\n",
    "In the cell below, we will define our **neural network** model as a class called \"CbowNet\".\n",
    "\n",
    "Using the PyTorch framework, neural network models are **subclasses** of the [**Module**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class.\n",
    "\n",
    "> PyTorch uses modules to represent neural networks. Modules are:\n",
    ">\n",
    "> * Building blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.\n",
    "> * Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n",
    "> * Easy to work with and transform. Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.\n",
    ">\n",
    "> From the [PyTorch v 2.1 Documentation](https://pytorch.org/docs/stable/notes/modules.html)\n",
    "\n",
    "Implement the model below woth the following architecture:\n",
    "\n",
    "1) An embedding (see: [nn.Embedding](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html)) **hidden layer** with \"EMBEDDING_SIZE\" neurons.\n",
    "   * Embedding layers are fully connected, like a linear layer, but are optimized for working with natural language data.\n",
    "3) A summation of each of the (WINDOW_SIZE-1) embeddings for the context words (see: [torch.sum](https://docs.pytorch.org/docs/stable/generated/torch.sum.html)).\n",
    "4) A linear **output layer** with \"OUTPUT_SIZE\" neurons that uses the log_softmax (see: [F.log_softmax](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)) activation function.\n",
    "   * Softmax can be used to convert a vector of raw outputs into a vector of probability scores.\n",
    "   * We need to take the log softmax, because the loss function we will be using expects log to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07e5003-6287-4618-b97f-bf35fee13376",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO: Define the size of the input to the neural network and the size of the output from the neural network.\n",
    "Hint: Review your lecture notes. What determines the size of the inputs and the size of the outputs?\n",
    "'''\n",
    "# START your code here\n",
    "INPUT_SIZE = vocabulary_size\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = vocabulary_size\n",
    "# STOP your code here\n",
    "\n",
    "# Define the structure of the neural netowrk\n",
    "class CbowNet(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super(CbowNet, self).__init__()\n",
    "\n",
    "        self.L1 = None # Hidden layer\n",
    "        self.LN = None # Output layer\n",
    "        \n",
    "        '''\n",
    "        TO DO: Instantiate the layers (L1-LN) according to the above description\n",
    "        '''\n",
    "        # START your code here\n",
    "        self.L1 = nn.Embedding(INPUT_SIZE, EMBEDDING_SIZE)\n",
    "        self.LN = nn.Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "        # STOP your code here\n",
    "\n",
    "    # Defines the steps taken during the \"forward pass\"\n",
    "    def forward(self, data):\n",
    "        '''\n",
    "        TO DO: \n",
    "         - The input layer is provided above\n",
    "         - Pass the data through the hidden and output layers, as defined above\n",
    "        '''\n",
    "        # START your code here\n",
    "        embeds = self.L1(data)\n",
    "        summed = torch.sum(embeds, dim=0)\n",
    "        out = self.LN(summed) \n",
    "        out = F.log_softmax(out, dim=0)\n",
    "        data = out.unsqueeze(0)  \n",
    "        # STOP your code here\n",
    "        \n",
    "        return data\n",
    "\n",
    "    # Performs a forward pass and returns the result with the highest probability\n",
    "    def predict(self, data):\n",
    "        probabilities = self.forward(data)\n",
    "        return torch.argmax(probabilities).item()\n",
    "        \n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_id[word]])\n",
    "        return self.L1(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e35b8-a405-4678-9c86-e49a72a52c62",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b57993-713c-4056-a6e3-2d888cba319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "CPU times: total: 2min 47s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^This will display how long it takes to execute this cell\n",
    "\n",
    "model = CbowNet()\n",
    "loss_function = nn.NLLLoss()\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), LEARNING_RATE)\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_error = 0\n",
    "\n",
    "    for context, target in input_batches:\n",
    "        X = context_words_to_ids(context, word_to_id)  \n",
    "        y_t = torch.tensor([word_to_id[target]])\n",
    "        y_p = model(X)\n",
    "        total_error += loss_function(y_p, y_t)\n",
    "\n",
    "    '''\n",
    "    NOTE:\n",
    "     - Here is where we start letting the optimizer calculate our derivitives for us!\n",
    "    TO DO: \n",
    "     - Use your optimizer to zero-out the gradients for the weights and biases. Otherwise they would accumulate over each batch.\n",
    "     - Using your total error function result, run the \"backpropagation\" or \"backwards pass\" through the model\n",
    "        - Error goes from Output Layer -> Hidden Layer N -> ... -> Hidden Layer 1 -> Input Layer\n",
    "     - Using your optimizer, Update the weights and biases\n",
    "    '''\n",
    "    # START your code here\n",
    "    optimizer.zero_grad()        # Clear gradients\n",
    "    total_error = total_error.unsqueeze(0)  # make it a 1×1 tensor to avoid size mismatch\n",
    "    total_error.backward()       # backpropagate\n",
    "    optimizer.step()             # update weights\n",
    "    # STOP your code here\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b384bf-598b-47d4-8992-e0566a86ffcd",
   "metadata": {},
   "source": [
    "## Application: Fill in the blank\n",
    "One use of our model is to predict a target work based on a given context.\n",
    "\n",
    "For our first prediction example, we will use a segment that exists in the input corpus: \"ribon of *black* and white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a938c81-fd44-4f4d-8b5d-d655f8c0b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['ribbon', 'of', 'and', 'white']\n",
      "Predicted: black\n",
      "Expected: black\n"
     ]
    }
   ],
   "source": [
    "context_words = ['ribbon', 'of', 'and', 'white']\n",
    "context_ids = context_words_to_ids(context_words, word_to_id)\n",
    "y_p = model.predict(context_ids)\n",
    "y_t = \"black\"\n",
    "\n",
    "print('Context: {}'.format(context_words))\n",
    "print('Predicted: {}'.format(id_to_word[y_p]))\n",
    "print('Expected: {}'.format(y_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ca291-c8e1-42ea-b7c0-aedb98b737c3",
   "metadata": {},
   "source": [
    "Next, we can give our model a novel context (which doesn't exist in the input corpus): \"loons are _____ consealed under\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e1ade5e-13f6-45e6-88ea-aab99acb8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['loons', 'are', 'concealed', 'under']\n",
      "Predicted: always\n"
     ]
    }
   ],
   "source": [
    "context_words = ['loons', 'are', 'concealed', 'under' ]\n",
    "context_ids = context_words_to_ids(context_words, word_to_id)\n",
    "y_p = model.predict(context_ids)\n",
    "\n",
    "print('Context: {}'.format(context_words))\n",
    "print('Predicted: {}'.format(id_to_word[y_p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22e1f8-c0ce-4451-872c-01e4c7da1f1c",
   "metadata": {},
   "source": [
    "## Finding synonyms\n",
    "Next, we will be looking at another way we can use embeddings from our model's hidden layer: finding synonyms.\n",
    "\n",
    "A synonym for a target word will be used in many of the same contexts as the target, therefore the learned embedding for both the target and its synonym will be close to eachother.\n",
    "\n",
    "Because our embedding layer size is 100, there will be 100 learned weights for each word.\n",
    "Let's take a look at what the embedding for the target word \"loons\" looks like, as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b7c5cff-468f-4a94-a63c-c1ff14b78c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example word: loons\n",
      "Embedding size: torch.Size([1, 100])\n",
      "tensor([[ 1.6877e+00, -2.4270e-01,  1.4254e+00, -3.6263e-01, -1.2473e-02,\n",
      "         -6.1673e-01, -1.9608e+00,  6.5924e-01,  2.2071e+00, -7.4053e-02,\n",
      "         -1.7828e+00, -7.7567e-01,  1.8969e+00,  9.2272e-01,  1.0088e+00,\n",
      "         -9.6745e-02,  9.7054e-01, -2.4015e-01,  1.6926e+00,  1.7361e-02,\n",
      "          1.2856e+00, -2.7721e-02, -4.9146e-02,  6.5882e-01,  1.2459e+00,\n",
      "         -3.1179e-01, -1.7870e+00, -1.5852e+00, -1.0139e+00,  1.6499e+00,\n",
      "         -1.3078e+00,  1.8393e-01, -2.5021e+00,  1.4890e+00,  7.9824e-01,\n",
      "          7.7647e-01,  5.5561e-01,  1.2233e+00,  9.1133e-01, -1.6318e+00,\n",
      "         -2.4021e+00,  7.9234e-01, -2.9244e-01,  1.4287e+00, -3.6585e-01,\n",
      "         -4.2307e-02,  4.7193e-01,  1.3239e+00, -1.0924e-01,  1.4369e-01,\n",
      "          1.4302e+00, -1.5916e+00,  2.3982e+00, -4.1991e-02, -1.8029e-01,\n",
      "          7.6279e-01, -8.8968e-02,  2.2973e+00,  1.8521e+00, -3.2098e-01,\n",
      "          6.8903e-04, -1.1925e+00, -6.3041e-01, -1.4235e+00,  2.2265e-01,\n",
      "         -1.4428e+00,  8.4636e-01, -1.7714e+00,  6.8834e-03, -1.2480e+00,\n",
      "          5.6557e-01, -2.6947e-02,  1.6663e+00, -4.4555e-01, -6.8111e-01,\n",
      "          8.2284e-01,  1.7885e-01,  1.2627e+00,  6.1569e-01,  1.2659e+00,\n",
      "         -1.4902e-01,  1.1693e+00,  2.9776e-01, -1.1003e+00, -4.6201e-01,\n",
      "          8.0792e-01, -1.5450e-01, -1.1691e+00,  5.3167e-01, -1.7483e+00,\n",
      "         -1.0064e+00,  1.0491e-01,  5.7537e-01, -2.4995e-01,  8.4108e-01,\n",
      "         -1.4965e-01, -3.0890e-01, -3.3267e-01, -1.1363e+00,  7.3647e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "example_word = \"loons\"\n",
    "example_embedding = model.get_word_emdedding(example_word)\n",
    "print(\"Example word: {}\".format(example_word))\n",
    "print(\"Embedding size: {}\".format(example_embedding.size()))\n",
    "print(example_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7f6f8-ff72-4ba6-97d0-7b8038fb5b90",
   "metadata": {},
   "source": [
    "One way to measure the \"closeness\" of these embeddings is by using a distance measure.\n",
    "\n",
    "For simplicity and familiarity, we will be using the [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance):\n",
    "\n",
    "$$d(p,q)=\\sqrt{\\sum_i^{\\text{features}}{(p_i - q_i)^2}}$$\n",
    "\n",
    "Implementation hint: if you would like to work with embeddings as numpy nd-arrays instead of as tensors, you can use the following functions.\n",
    "     * https://docs.pytorch.org/docs/stable/generated/torch.Tensor.detach.html\n",
    "     * https://docs.pytorch.org/docs/stable/generated/torch.Tensor.numpy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8b967df-fe48-42b3-a11f-4102aeede422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Calculates the euclidean distance between two word embeddings\n",
    "def Distance(target, other):\n",
    "    distance = None\n",
    "    \n",
    "    '''\n",
    "    TO DO: Implement the Euclidean Distance equation\n",
    "    '''\n",
    "    # START your code here\n",
    "    distance = torch.sqrt(torch.sum((target.squeeze() - other.squeeze()) ** 2)).item()\n",
    "    # STOP your code here\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9406e561-dbf6-45d5-bcf7-e22f3334703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the word with the minimum distance from the target word\n",
    "def MinDistance(target, others):\n",
    "    min_idx = None\n",
    "\n",
    "    '''\n",
    "    TO DO: \n",
    "    1. Get all distances (from the target word to each of the other words)\n",
    "    2. Get the index of the minimum distance\n",
    "    '''\n",
    "    # START your code here\n",
    "    target_embedding = model.get_word_emdedding(target)\n",
    "    other_embeddings = [model.get_word_emdedding(word) for word in others]\n",
    "    \n",
    "    distances = [Distance(target_embedding, other_emb) for other_emb in other_embeddings]\n",
    "    min_idx = distances.index(min(distances))\n",
    "    # STOP your code here\n",
    "    \n",
    "    return others[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df56732-24ce-4087-90ba-f8beebe7d830",
   "metadata": {},
   "source": [
    "Use this last cell to change the target word to any word in our vocabulary. \n",
    "Experiment with different target words and see what synonyms our model comes up with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47998dba-b8b2-47e9-9f28-e10ee5e224af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target word: always\n",
      "Synonym: known\n"
     ]
    }
   ],
   "source": [
    "# START your code here\n",
    "target_word = \"always\" # Choose any word from the vocabulary\n",
    "# STOP your code here\n",
    "\n",
    "# Find the closest synonym, using the minimum distance from all other vocabulary words\n",
    "other_words = list(vocabulary.copy())\n",
    "other_words.remove(target_word)\n",
    "synonym = MinDistance(target_word, other_words)\n",
    "\n",
    "print(\"Target word: {}\".format(target_word))\n",
    "print(\"Synonym: {}\".format(synonym))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610eb1f-7a4e-4dbb-9420-32109bf44ea8",
   "metadata": {},
   "source": [
    "Congratulations, you have reached the end of this notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
